{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd0e906-5f31-4da6-b441-368137c40b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download, notebook_login, login\n",
    "import numpy as np\n",
    "\n",
    "from sae_lens.toolkit.pretrained_saes_directory import get_pretrained_saes_directory\n",
    "from sae_lens import SAE, HookedSAETransformer\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43abb34-cfd4-49ec-b026-8103c483c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cmlscratch/sriramb/anaconda3/envs/mechinterp/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedSAETransformer.from_pretrained(\"gpt2-small\", device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee5e390c-19e1-46bf-82e8-a822ee832ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "ACTIVATIONS_SIZE = 768\n",
    "HOOK_POINT = 'blocks.8.hook_resid_pre'\n",
    "THRESHOLD_INIT = 0.001\n",
    "BANDWIDTH = 0.001\n",
    "FIX_DECODER_NORMS = True\n",
    "LEARNING_RATE = 0.001  # Note this is not the learning rate in the paper\n",
    "ADAM_B1 = 0.0\n",
    "DATA_SEED = 9328302\n",
    "PARAMS_SEED = 24396\n",
    "rng = np.random.default_rng(DATA_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14abb7da-1ade-4768-afe8-84e15d1b8266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rectangle_pt(x):\n",
    "    return ((x > -0.5) & (x < 0.5)).to(x)\n",
    "\n",
    "\n",
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, threshold):\n",
    "        return (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = 0.0 * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(1.0 / BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n",
    "\n",
    "\n",
    "class JumpReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, threshold):\n",
    "        return x * (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(threshold / BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n",
    "\n",
    "\n",
    "class Sae(nn.Module):\n",
    "    def __init__(self, sae_width, activations_size, use_pre_enc_bias):\n",
    "        super().__init__()\n",
    "        self.dtype = torch.float\n",
    "        self.device = device\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        self.W_enc = nn.Parameter(\n",
    "            torch.nn.init.kaiming_uniform_(\n",
    "                torch.empty(\n",
    "                    activations_size, sae_width, dtype=self.dtype, device=self.device\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.b_enc = nn.Parameter(\n",
    "            torch.zeros(sae_width, dtype=self.dtype, device=self.device)\n",
    "        )\n",
    "        self.W_dec = nn.Parameter(\n",
    "            self.W_enc.data.T\n",
    "            # torch.nn.init.kaiming_uniform_(\n",
    "            #     torch.empty(\n",
    "            #         sae_width, activations_size, dtype=self.dtype, device=self.device\n",
    "            #     )\n",
    "            # )\n",
    "        )\n",
    "        self.b_dec = nn.Parameter(\n",
    "            torch.zeros(activations_size, dtype=self.dtype, device=self.device)\n",
    "        )\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            np.log(THRESHOLD_INIT)*torch.ones(sae_width, dtype=self.dtype, device=self.device)\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.b_dec\n",
    "\n",
    "        pre_activations = x @ self.W_enc + self.b_enc\n",
    "        # if self.use_jumprelu:\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        feature_magnitudes = JumpReLU.apply(pre_activations, threshold)\n",
    "        # else:\n",
    "        #     feature_magnitudes = F.relu(pre_activations)\n",
    "        \n",
    "        x_reconstructed = feature_magnitudes @ self.W_dec + self.b_dec\n",
    "        return x_reconstructed, pre_activations\n",
    "\n",
    "\n",
    "def loss_fn_pt(sae, x, sparsity_coefficient):\n",
    "    x_reconstructed, pre_activations = sae(x)\n",
    "\n",
    "    # Compute per-example reconstruction loss\n",
    "    reconstruction_error = x - x_reconstructed\n",
    "    reconstruction_loss = torch.sum(reconstruction_error**2, dim=-1)\n",
    "\n",
    "    # Compute per-example sparsity loss\n",
    "    threshold = torch.exp(sae.log_threshold)\n",
    "    l0 = torch.sum(Step.apply(pre_activations, threshold), dim=-1)\n",
    "    sparsity_loss = sparsity_coefficient * l0\n",
    "\n",
    "    # Return the batch-wise mean total loss\n",
    "    return reconstruction_loss.mean(), sparsity_loss.mean()\n",
    "\n",
    "\n",
    "def remove_parallel_component_pt(x, v):\n",
    "    \"\"\"Returns x with component parallel to v projected away (in PyTorch).\"\"\"\n",
    "    v_normalised = v / (torch.norm(v, dim=-1, keepdim=True) + 1e-6)\n",
    "    parallel_component = torch.einsum(\"...d,...d->...\", x, v_normalised)\n",
    "    return x - parallel_component[..., None] * v_normalised\n",
    "\n",
    "def train_pt(\n",
    "    sae, \n",
    "    optimizer,\n",
    "    dataloader,\n",
    "    sparsity_coefficient,\n",
    "    num_steps = 1\n",
    "):\n",
    "    \n",
    "    for i, tokens in enumerate(pbar := tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            _, cache = model.run_with_cache(tokens['tokens'], names_filter = [HOOK_POINT], stop_at_layer = 9,)\n",
    "            norm_res = F.normalize(cache[HOOK_POINT], dim=-1)\n",
    "            # norm_res = cache[HOOK_POINT]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        recon_loss, sparsity_loss = loss_fn_pt(\n",
    "            sae, norm_res, sparsity_coefficient\n",
    "        )\n",
    "        loss_pt = recon_loss + sparsity_loss\n",
    "        loss_pt.backward()\n",
    "\n",
    "        if FIX_DECODER_NORMS:\n",
    "            sae.W_dec.grad = remove_parallel_component_pt(\n",
    "                sae.W_dec.grad, sae.W_dec.data\n",
    "            )\n",
    "        optimizer.step()\n",
    "        if FIX_DECODER_NORMS:\n",
    "            sae.W_dec.data = sae.W_dec.data / torch.norm(\n",
    "                sae.W_dec.data, dim=-1, keepdim=True\n",
    "            )\n",
    "\n",
    "        pbar.set_description_str(f'recon loss: {recon_loss.item()} , sparsity loss: {sparsity_loss.item()}')\n",
    "\n",
    "        if i == num_steps:\n",
    "            break\n",
    "    return sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4a3c860-6d1f-40c4-8df9-acd997a3f082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title STEs, forward pass and loss function\n",
    "from datasets import load_dataset\n",
    "from transformer_lens.utils import tokenize_and_concatenate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = load_dataset(\n",
    "    path = \"Skylion007/openwebtext\",\n",
    "    split=\"train[0:200000]\",\n",
    "    streaming=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc2f4f58-73f3-4007-b618-7082dbcd7efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = tokenize_and_concatenate(\n",
    "    dataset= dataset,\n",
    "    tokenizer = model.tokenizer,\n",
    "    streaming=True,\n",
    "    max_length=128,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(token_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88d9a9d-1ab3-4f91-841b-c60ff13f8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparsity_coefficient = 0.01\n",
    "\n",
    "# sae = Sae(768, ACTIVATIONS_SIZE, False)\n",
    "\n",
    "# for i, tokens in enumerate(pbar := tqdm(dataloader)):\n",
    "#     with torch.no_grad():\n",
    "#         _, cache = model.run_with_cache(tokens['tokens'], names_filter = [HOOK_POINT], stop_at_layer = 9,)\n",
    "#         norm_res = F.normalize(cache[HOOK_POINT], dim=-1)\n",
    "\n",
    "# x_reconstructed, pre_activations = sae(norm_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57964f00-d36e-476c-bebf-ccb863466e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9798e203adcf4abeb3c24255f5f1174b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sparsity_coefficient =  0.01\n",
    "\n",
    "jump_sae_768 = Sae(768, ACTIVATIONS_SIZE, False)\n",
    "optimizer = torch.optim.Adam(\n",
    "    jump_sae_768.parameters(), lr=LEARNING_RATE, betas=(ADAM_B1, 0.999)\n",
    ")\n",
    "jump_sae_768 = train_pt(jump_sae_768, optimizer, dataloader, sparsity_coefficient, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc13daf0-e64d-41f1-8af0-39b44464032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(jump_sae_768.cpu().state_dict(), './jump_sae_768-final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ad5cde7-0f97-4ce9-b6c6-3b484eb07c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ccd7ab1f28463ca23cf9264481b35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparsity_coefficient = 0.005\n",
    "\n",
    "jump_sae_1536 = Sae(1536, ACTIVATIONS_SIZE, False)\n",
    "optimizer = torch.optim.Adam(\n",
    "    jump_sae_1536.parameters(), lr=LEARNING_RATE, betas=(ADAM_B1, 0.999)\n",
    ")\n",
    "\n",
    "jump_sae_1536 = train_pt(jump_sae_1536, optimizer, dataloader, sparsity_coefficient, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c4190-ae0b-41e1-8de7-67fbfbfd3874",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cceda6d-6ce7-4823-b735-141f29121649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6c7ca133de4011a55481b449277489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1743 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(\n\u001b[1;32m      3\u001b[0m     jump_sae_1536\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE, betas\u001b[38;5;241m=\u001b[39m(ADAM_B1, \u001b[38;5;241m0.999\u001b[39m)\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m sparsity_coefficient \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[0;32m----> 6\u001b[0m jump_sae_1536 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_pt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjump_sae_1536\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparsity_coefficient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 153\u001b[0m, in \u001b[0;36mtrain_pt\u001b[0;34m(sae, optimizer, dataloader, sparsity_coefficient, num_steps)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FIX_DECODER_NORMS:\n\u001b[1;32m    149\u001b[0m     sae\u001b[38;5;241m.\u001b[39mW_dec\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m sae\u001b[38;5;241m.\u001b[39mW_dec\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(\n\u001b[1;32m    150\u001b[0m         sae\u001b[38;5;241m.\u001b[39mW_dec\u001b[38;5;241m.\u001b[39mdata, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description_str(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecon loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mrecon_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , sparsity loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msparsity_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m num_steps:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "jump_sae_1536.to(device)\n",
    "optimizer = torch.optim.Adam(\n",
    "    jump_sae_1536.parameters(), lr=LEARNING_RATE, betas=(ADAM_B1, 0.999)\n",
    ")\n",
    "sparsity_coefficient = 0.001\n",
    "jump_sae_1536 = train_pt(jump_sae_1536, optimizer, dataloader, sparsity_coefficient, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afe027a6-33a9-4de3-ae5e-496697650ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(jump_sae_1536.cpu().state_dict(), './jump_sae_1536-final.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
