{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1rvyLNB_Z4r"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook provides reference implementations for [JumpReLU SAEs](https://arxiv.org/abs/2407.14435) in JAX and PyTorch, expanding on the pseudo-code provided in the paper. Specifically, we include:\n",
    "\n",
    "* Implementations of the `jumprelu` and `step` functions with custom backward passes;\n",
    "* Implementations of the SAE forward pass and L0-based loss function;\n",
    "* Training loop implementations that optionally normalise the norms of the decoder matrix.\n",
    "\n",
    "We don't implement some features used in the training setup described in the paper (e.g. learning rate and sparsity coefficient $\\lambda$ warm-up) which are reasonably easy to add on if desired.\n",
    "\n",
    "The notebook also provides comprehensive tests to check that the Jax and PyTorch implementations are consistent. This includes an end-to-end training test where we train SAEs (on synthetic data, using identical initialisations) using both the Jax and PyTorch implementations and check we get the similar parameters after three steps. You may find these tests useful for testing other implementations of JumpReLU SAEs against these reference implementations to confirm consistency.\n",
    "\n",
    "You should be able to run this notebook on a CPU runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PejpwsV3B_k6"
   },
   "source": [
    "## Setup\n",
    "\n",
    "This section imports modules needed by the rest of the notebook, sets some constants (mainly hyperparameters) and generates some synthetic data and initialises some SAE parameters that we use later for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "PhoS7Jik9FkJ"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "import dataclasses\n",
    "import functools\n",
    "import itertools\n",
    "\n",
    "import chex\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "B-2paQs89ag6"
   },
   "outputs": [],
   "source": [
    "# @title Hyperparameters and constants\n",
    "\n",
    "NUM_STEPS = 3\n",
    "BATCH_SIZE = 1024\n",
    "ACTIVATIONS_SIZE = 16\n",
    "SAE_WIDTH = 128\n",
    "THRESHOLD_INIT = 0.001\n",
    "# We use a higher bandwidth than in the paper to ensure a non-zero gradient\n",
    "# to the threshold at every step (since we'll only be taking three steps)\n",
    "BANDWIDTH = 0.1\n",
    "FIX_DECODER_NORMS = True\n",
    "LEARNING_RATE = 0.001  # Note this is not the learning rate in the paper\n",
    "ADAM_B1 = 0.0\n",
    "DATA_SEED = 9328302\n",
    "PARAMS_SEED = 24396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "6UsubY3D9OpO"
   },
   "outputs": [],
   "source": [
    "# @title Create some synthetic data for testing\n",
    "\n",
    "rng = np.random.default_rng(DATA_SEED)\n",
    "dataset = rng.normal(\n",
    "    size=(NUM_STEPS, BATCH_SIZE, ACTIVATIONS_SIZE)\n",
    ") / np.sqrt(ACTIVATIONS_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "q0zFe2dD-CVv"
   },
   "outputs": [],
   "source": [
    "# @title Choose random SAE weights for testing\n",
    "\n",
    "# We choose an initialization that is useful for testing. Specifically\n",
    "# this means we initialize the biases and threshold to non-zero values\n",
    "# and that we don't set the encoder weights to the transpose of the decoder\n",
    "# (since they won't in general during training).\n",
    "rng = np.random.default_rng(PARAMS_SEED)\n",
    "W_dec = (rng.uniform(size=(SAE_WIDTH, ACTIVATIONS_SIZE)) - 0.5)\n",
    "W_dec /= np.linalg.norm(W_dec, axis=-1, keepdims=True)\n",
    "W_enc = (rng.uniform(size=(ACTIVATIONS_SIZE, SAE_WIDTH)) - 0.5)\n",
    "b_enc = (rng.uniform(size=(SAE_WIDTH,)) - 0.5) * 0.1\n",
    "b_dec = (rng.uniform(size=(ACTIVATIONS_SIZE,)) - 0.5) * 0.1\n",
    "threshold = 0.15 * (rng.uniform(size=(SAE_WIDTH,))) * 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sHRICVB_hyaw"
   },
   "source": [
    "## PyTorch implementation\n",
    "\n",
    "In this section we translate the JAX implementation defined in the previous section into PyTorch. We'll then check carefully that the PyTorch implementation is consistent with the JAX one, the key test being that training over multiple steps with either implementation (using synthetic data and identical initialisation) yields the same parameters (up to numerical tolerance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "\n",
    "\n",
    "def rectangle_pt(x: torch.Tensor) -> torch.Tensor:\n",
    "    return ((x > -0.5) & (x < 0.5)).to(x)\n",
    "\n",
    "\n",
    "class Step2(torch.autograd.Function):\n",
    "    BANDWIDTH = 0.001\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "        return (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(\n",
    "        ctx: Any, inputs: Tuple[torch.Tensor, torch.Tensor], output: torch.Tensor\n",
    "    ) -> None:\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx: Any, grad_output: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = 0.0 * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(1.0 / Step2.BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / Step2.BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n",
    "\n",
    "\n",
    "class JumpReLU2(torch.autograd.Function):\n",
    "    BANDWIDTH = 0.001\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(x: torch.Tensor, threshold: torch.Tensor) -> torch.Tensor:\n",
    "        return x * (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(\n",
    "        ctx: Any, inputs: Tuple[torch.Tensor, torch.Tensor], output: torch.Tensor\n",
    "    ) -> None:\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(\n",
    "        ctx: Any, grad_output: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(threshold / JumpReLU2.BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / JumpReLU2.BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n",
    "\n",
    "Step2.BANDWIDTH = BANDWIDTH\n",
    "JumpReLU2.BANDWIDTH = BANDWIDTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "j6KFi16Pht4a"
   },
   "outputs": [],
   "source": [
    "# @title STEs, forward pass and loss function\n",
    "\n",
    "def rectangle_pt(x):\n",
    "    return ((x > -0.5) & (x < 0.5)).to(x)\n",
    "\n",
    "\n",
    "class Step(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, threshold):\n",
    "        return (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = 0.0 * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(1.0 / BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n",
    "\n",
    "\n",
    "class JumpReLU(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(x, threshold):\n",
    "        return x * (x > threshold).to(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def setup_context(ctx, inputs, output):\n",
    "        x, threshold = inputs\n",
    "        del output\n",
    "        ctx.save_for_backward(x, threshold)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, threshold = ctx.saved_tensors\n",
    "        x_grad = (x > threshold) * grad_output  # We don't apply STE to x input\n",
    "        threshold_grad = torch.sum(\n",
    "            -(threshold / BANDWIDTH)\n",
    "            * rectangle_pt((x - threshold) / BANDWIDTH)\n",
    "            * grad_output,\n",
    "            dim=0,\n",
    "        )\n",
    "        return x_grad, threshold_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sae(nn.Module):\n",
    "    def __init__(self, sae_width, activations_size, use_pre_enc_bias, mode):\n",
    "        super().__init__()\n",
    "        self.use_pre_enc_bias = use_pre_enc_bias\n",
    "        self.W_enc = nn.Parameter(torch.tensor(W_enc))\n",
    "        self.b_enc = nn.Parameter(torch.tensor(b_enc))\n",
    "        self.W_dec = nn.Parameter(torch.tensor(W_dec))\n",
    "        self.b_dec = nn.Parameter(torch.tensor(b_dec))\n",
    "        self.log_threshold = nn.Parameter(\n",
    "            torch.tensor(np.log(threshold))\n",
    "        )\n",
    "        self.mode = mode\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.use_pre_enc_bias:\n",
    "            x = x - self.b_dec\n",
    "\n",
    "        pre_activations = x @ self.W_enc + self.b_enc\n",
    "        threshold = torch.exp(self.log_threshold)\n",
    "        if self.mode == \"1\":\n",
    "            feature_magnitudes = JumpReLU.apply(pre_activations, threshold)\n",
    "        else:\n",
    "            feature_magnitudes = JumpReLU2.apply(pre_activations, threshold)\n",
    "        x_reconstructed = feature_magnitudes @ self.W_dec + self.b_dec\n",
    "        return x_reconstructed, pre_activations\n",
    "\n",
    "\n",
    "def loss_fn_pt(sae, x, sparsity_coefficient, use_pre_enc_bias, mode):\n",
    "    x_reconstructed, pre_activations = sae(x)\n",
    "\n",
    "    # Compute per-example reconstruction loss\n",
    "    reconstruction_error = x - x_reconstructed\n",
    "    reconstruction_loss = torch.sum(reconstruction_error**2, dim=-1)\n",
    "\n",
    "    # Compute per-example sparsity loss\n",
    "    threshold = torch.exp(sae.log_threshold)\n",
    "\n",
    "    if mode == \"1\":\n",
    "        l0 = torch.sum(Step.apply(pre_activations, threshold), dim=-1)\n",
    "    else:\n",
    "        l0 = torch.sum(Step2.apply(pre_activations, threshold), dim=-1)\n",
    "    sparsity_loss = sparsity_coefficient * l0\n",
    "\n",
    "    # Return the batch-wise mean total loss\n",
    "    return torch.mean(reconstruction_loss + sparsity_loss, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "form",
    "id": "Zz0TwpNCuDOP"
   },
   "outputs": [],
   "source": [
    "# @title Training loop\n",
    "\n",
    "def remove_parallel_component_pt(x, v):\n",
    "    \"\"\"Returns x with component parallel to v projected away (in PyTorch).\"\"\"\n",
    "    v_normalised = v / (torch.norm(v, dim=-1, keepdim=True) + 1e-6)\n",
    "    parallel_component = torch.einsum(\"...d,...d->...\", x, v_normalised)\n",
    "    return x - parallel_component[..., None] * v_normalised\n",
    "\n",
    "def train_pt(\n",
    "    dataset_iterator,\n",
    "    sparsity_coefficient,\n",
    "    use_pre_enc_bias,\n",
    "    fix_decoder_norms,\n",
    "    mode\n",
    "):\n",
    "    sae = Sae(SAE_WIDTH, ACTIVATIONS_SIZE, use_pre_enc_bias, mode)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        sae.parameters(), lr=LEARNING_RATE, betas=(ADAM_B1, 0.999)\n",
    "    )\n",
    "    for batch in dataset_iterator:\n",
    "        optimizer.zero_grad()\n",
    "        loss_pt = loss_fn_pt(\n",
    "            sae, torch.tensor(batch), sparsity_coefficient, use_pre_enc_bias, mode\n",
    "        )\n",
    "        loss_pt.backward()\n",
    "        if fix_decoder_norms:\n",
    "            sae.W_dec.grad = remove_parallel_component_pt(\n",
    "                sae.W_dec.grad, sae.W_dec.data\n",
    "            )\n",
    "        optimizer.step()\n",
    "        if fix_decoder_norms:\n",
    "            sae.W_dec.data = sae.W_dec.data / torch.norm(\n",
    "                sae.W_dec.data, dim=-1, keepdim=True\n",
    "            )\n",
    "    return sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yIRU1pjmQJT",
    "outputId": "2bc9ddd6-0b8d-4381-d5cf-9bd0bdce777e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing sparsity_coefficient=0.0, use_pre_enc_bias=True, fix_decoder_norms=True... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'params_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 47\u001b[0m\n\u001b[1;32m     34\u001b[0m sae_pt_trained_2 \u001b[38;5;241m=\u001b[39m train_pt(\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28miter\u001b[39m(dataset),\n\u001b[1;32m     36\u001b[0m     sparsity_coefficient,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# First we want to make sure the params have actually evolved, otherwise\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# this test isn't meaningful!\u001b[39;00m\n\u001b[1;32m     44\u001b[0m chex\u001b[38;5;241m.\u001b[39massert_trees_all_close(\n\u001b[1;32m     45\u001b[0m     jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mlambda\u001b[39;00m x, y: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(x \u001b[38;5;241m-\u001b[39m y)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m---> 47\u001b[0m         \u001b[43mparams_init\u001b[49m,\n\u001b[1;32m     48\u001b[0m         params_jax_trained,\n\u001b[1;32m     49\u001b[0m     ),\n\u001b[1;32m     50\u001b[0m     jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m _: \u001b[38;5;28;01mTrue\u001b[39;00m, params_init),\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Now we check whether the parameters obtained using either implementation\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# are close\u001b[39;00m\n\u001b[1;32m     55\u001b[0m chex\u001b[38;5;241m.\u001b[39massert_trees_all_close(\n\u001b[1;32m     56\u001b[0m     jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28mdict\u001b[39m(sae_pt_trained_1\u001b[38;5;241m.\u001b[39mstate_dict())),\n\u001b[1;32m     57\u001b[0m     jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mnumpy(), \u001b[38;5;28mdict\u001b[39m(sae_pt_trained_2\u001b[38;5;241m.\u001b[39mstate_dict())),\n\u001b[1;32m     58\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'params_init' is not defined"
     ]
    }
   ],
   "source": [
    "sparsity_coefficients = [0.0, 0.01, 0.1]  # Arbitrarily chosen\n",
    "use_pre_enc_bias_l = [True, False]\n",
    "fix_decoder_norms_l = [True, False]\n",
    "\n",
    "\n",
    "for sparsity_coefficient, use_pre_enc_bias, fix_decoder_norms in itertools.product(\n",
    "    sparsity_coefficients, use_pre_enc_bias_l, fix_decoder_norms_l\n",
    "):\n",
    "    print(\n",
    "        f\"Testing {sparsity_coefficient=}, {use_pre_enc_bias=}, \"\n",
    "        f\"{fix_decoder_norms=}... \",\n",
    "        end=\"\",\n",
    "        flush=True,\n",
    "    )\n",
    "\n",
    "    # Train using the JAX implementation\n",
    "    # params_jax_trained = train_jax(\n",
    "    #     iter(dataset),\n",
    "    #     sparsity_coefficient=sparsity_coefficient,\n",
    "    #     use_pre_enc_bias=use_pre_enc_bias,\n",
    "    #     fix_decoder_norms=fix_decoder_norms,\n",
    "    # )\n",
    "\n",
    "    # Train using the PyTorch implementation\n",
    "    sae_pt_trained_1 = train_pt(\n",
    "        iter(dataset),\n",
    "        sparsity_coefficient,\n",
    "        use_pre_enc_bias,\n",
    "        fix_decoder_norms,\n",
    "        \"1\"\n",
    "    )\n",
    "\n",
    "    # Train using the PyTorch implementation\n",
    "    sae_pt_trained_2 = train_pt(\n",
    "        iter(dataset),\n",
    "        sparsity_coefficient,\n",
    "        use_pre_enc_bias,\n",
    "        fix_decoder_norms,\n",
    "        \"2\"\n",
    "    )\n",
    "\n",
    "    # First we want to make sure the params have actually evolved, otherwise\n",
    "    # this test isn't meaningful!\n",
    "    # chex.assert_trees_all_close(\n",
    "    #     jax.tree.map(\n",
    "    #         lambda x, y: np.mean(np.abs(x - y)) > 0.001,\n",
    "    #         params_init,\n",
    "    #         params_jax_trained,\n",
    "    #     ),\n",
    "    #     jax.tree.map(lambda _: True, params_init),\n",
    "    # )\n",
    "\n",
    "    # Now we check whether the parameters obtained using either implementation\n",
    "    # are close\n",
    "    chex.assert_trees_all_close(\n",
    "        jax.tree.map(lambda x: x.numpy(), dict(sae_pt_trained_1.state_dict())),\n",
    "        jax.tree.map(lambda x: x.numpy(), dict(sae_pt_trained_2.state_dict())),\n",
    "    )\n",
    "\n",
    "    print(\"OK.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
